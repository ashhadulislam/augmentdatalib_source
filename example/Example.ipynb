{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Part I: Running KNNOR","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nimport numpy as np\nimport collections\nfrom knnor import data_augment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_breast_cancer()\n\n(unique, counts) = np.unique(dataset['target'], return_counts=True)\n\nprint('Unique values of the target variable', unique)\nprint('Counts of the target variable :', counts)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=dataset[\"data\"]\ny=dataset[\"target\"]\n\nprint(\"Original shape=\",X.shape,y.shape)\nelements_count = collections.Counter(y)\n# printing the element and the frequency\nprint(\"Original distribution:\")\nfor key, value in elements_count.items():\n    print(f\"{key}: {value}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Augmentation without any parameters\n\nThe algorithm calculates the parameters depending on the data\n\nFinal result will give an equal number of minority and majority data points\n","metadata":{}},{"cell_type":"code","source":"knnor=data_augment.KNNOR()\nX_new,y_new,_,_=knnor.fit_resample(X,y)\nprint(\"Shape after augmentation\",X_new.shape,y_new.shape)\nelements_count = collections.Counter(y_new)\n# printing the element and the frequency\nprint(\"Final distribution:\")\nfor key, value in elements_count.items():\n    print(f\"{key}: {value}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentation with user defined parameters\n","metadata":{}},{"cell_type":"code","source":"X_new,y_new,_,_=knnor.fit_resample(X,y,\n                              num_neighbors=10, # the number of neighbors that will be used for generation of each artificial point\n                              max_dist_point=0.01, # the maximum distance at which the new point will be placed\n                              proportion_minority=0.3, # proportion of the minority population that will be used to generate the artificial point\n                              final_proportion=2 # final number of minority datapoints\n                               # example, if num majority =15 and num minority =5, \n#                                putting final_proportion as 1 will add 10 artificial minority points\n                              )\nprint(\"Shape after augmentation\",X_new.shape,y_new.shape)\nelements_count = collections.Counter(y_new)\n# printing the element and the frequency\nprint(\"Final distribution:\")\nfor key, value in elements_count.items():\n    print(f\"{key}: {value}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part II: Testing with benchmark datasets against state-of-art oversamplers","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport itertools\nimport os.path\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom smote_variant import MLPClassifierWrapper\nfrom sklearn import svm\nimport pickle\n\nimport imbalanced_databases as imbd\n# library containing the imbalanced datasets\n# datasets will be present in the \"/data\" folder\n# in case the data is not already there\n# the above library will be used to download and save as pickle\n\n\nimport smote_variant as sv\n# the library containing oversampling code\n# includes first version of knnor as well\n# as other state of art oversamplers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the cache_path which is used for caching during the evaluation\nprint(\"At\",os.getcwd())\ncache_path= os.path.join(os.path.dirname(os.getcwd()), 'results')\nprint(cache_path)\nif not os.path.exists(cache_path):\n    os.makedirs(cache_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Specifying the classifiers used for evaluation\n\n# instantiate classifiers\n\n# Support Vector Classifiers with 6 parameter combinations\nsv_classifiers= [CalibratedClassifierCV(LinearSVC(C=1.0, penalty='l1', loss= 'squared_hinge', dual= False)),\n                 CalibratedClassifierCV(LinearSVC(C=1.0, penalty='l2', loss= 'hinge', dual= True)),\n                 CalibratedClassifierCV(LinearSVC(C=1.0, penalty='l2', loss= 'squared_hinge', dual= False)),\n                 CalibratedClassifierCV(LinearSVC(C=10.0, penalty='l1', loss= 'squared_hinge', dual= False)),\n                 CalibratedClassifierCV(LinearSVC(C=10.0, penalty='l2', loss= 'hinge', dual= True)),\n                 CalibratedClassifierCV(LinearSVC(C=10.0, penalty='l2', loss= 'squared_hinge', dual= False))]\n\n# Multilayer Perceptron Classifiers with 6 parameter combinations\nmlp_classifiers= []\nfor x in itertools.product(['relu', 'logistic'], [1.0, 0.5, 0.1]):\n    mlp_classifiers.append(MLPClassifierWrapper(activation= x[0], hidden_layer_fraction= x[1]))\n\n# Nearest Neighbor Classifiers with 18 parameter combinations\nnn_classifiers= []\nfor x in itertools.product([3, 5, 7], ['uniform', 'distance'], [1, 2, 3]):\n    nn_classifiers.append(KNeighborsClassifier(n_neighbors= x[0], weights= x[1], p= x[2]))\n\n# Decision Tree Classifiers with 6 parameter combinations\ndt_classifiers= []\nfor x in itertools.product(['gini', 'entropy'], [None, 3, 5]):\n    dt_classifiers.append(DecisionTreeClassifier(criterion= x[0], max_depth= x[1]))\n\nclassifiers= []\nclassifiers.extend(sv_classifiers)\nclassifiers.extend(mlp_classifiers)\nclassifiers.extend(nn_classifiers)\nclassifiers.extend(dt_classifiers)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncomment from below list to include\n# more augmentation algorithms\noversamplers= [    \n    sv.SMOTE,\n    sv.NoSMOTE,\n    sv.KNNOR_SMOTE,\n    sv.polynom_fit_SMOTE,\n    sv.ProWSyn,\n    sv.SMOTE_IPF,\n    sv.Lee,\n    sv.SMOBD,\n    sv.G_SMOTE,\n    sv.CCR,\n    sv.LVQ_SMOTE,\n    sv.Assembled_SMOTE,    \n    sv.SMOTE_TomekLinks\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if pickle file for data exists\nif os.path.isfile('../data/sampled_datasets_below_1000.p'):\n    sampled_datasets_below_1000 = pickle.load( open( \"../data/sampled_datasets_below_1000.p\", \"rb\" ) )\nelse:\n    sampled_datasets_below_1000=[\n        imbd.load_ecoli_0_1_3_7_vs_2_6,\n        imbd.load_pima,\n        imbd.load_cm1,\n        imbd.load_vowel0,\n        imbd.load_glass_0_1_6_vs_2,\n        imbd.load_yeast_0_5_6_7_9_vs_4,\n        imbd.load_yeast_1_vs_7,\n        imbd.load_ecoli_0_3_4_7_vs_5_6,\n        imbd.load_cleveland_0_vs_4,\n        imbd.load_iris0,\n        imbd.load_ecoli_0_6_7_vs_5,\n        imbd.load_winequality_white_3_vs_7,\n        imbd.load_ecoli_0_1_4_7_vs_2_3_5_6,\n        imbd.load_ecoli_0_3_4_vs_5,\n        imbd.load_glass0,\n        imbd.load_habarman,\n        imbd.load_glass_0_1_2_3_vs_4_5_6,\n        imbd.load_ecoli_0_6_7_vs_3_5,\n        imbd.load_poker_9_vs_7,\n    ]\n    pickle.dump( sampled_datasets_below_1000, open( \"../data/sampled_datasets_below_1000.p\", \"wb\" ) )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_files=len(sampled_datasets_below_1000)\nprint(\"total files=\",count_files)\n\n# comment below lines when running full version\nsamples_used=1\nsampled_datasets_below_1000=sampled_datasets_below_1000[:samples_used]\nprint(\"files used in this round of experiment\",len(sampled_datasets_below_1000))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart=int(time.time())\nmax_samp_par_comb=1 #50\n# change above to a higher value\n# for more trial runs\n\n\nresults= sv.evaluate_oversamplers(datasets= sampled_datasets_below_1000,\n                                    samplers= oversamplers,\n                                    classifiers=classifiers,\n                                    cache_path= cache_path,\n                                    n_jobs= 12,\n                                    max_samp_par_comb= max_samp_par_comb)\nduration=int(time.time())-start\nprint(\"Time taken = \",duration,\" seconds\")\nresults.to_csv(os.path.join(cache_path,\"Results.csv\"),index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Calculate time taken on average","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_speed=results.groupby('sampler')['runtime'].mean()\ndf_speed=df_speed.sort_values()\ndf_speed.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interesting_cols=[\"db_name\",\"classifier\",\"sampler\",\"auc\",\"f1\",\"p_top20\",\"gacc\"]\n\ndf = results[interesting_cols]\ndf=df.replace(\"KNNOR_SMOTE\",\"KNNOR\")\nsamplers=df[\"sampler\"].unique()\ncount_samplers=len(samplers)\nprint(\"Number of samplers:\",count_samplers,\"\\n\",samplers)\n\nclassifiers=df[\"classifier\"].unique()\ncount_classifiers=len(classifiers)\nprint(\"Number of classifiers:\",count_classifiers,\"\\n\",classifiers)\n\n\ndb_names=df[\"db_name\"].unique()\ncount_db_names=len(db_names)\nprint(\"Number of db_names:\",count_db_names,\"\\n\",db_names)\n\n\nmetrics=list(df.columns[-4:])\ncount_metrics=len(metrics)\nprint(\"Number of metrics:\",count_metrics,\"\\n\",metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for metric in metrics:\n#     df[metric+\"_rank\"]=[0 for i in range(df.shape[0])]\n    df[metric+\"_rank\"]=df.groupby(['db_name','classifier'])[metric].rank(ascending=False,method=\"min\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_excel(os.path.join(cache_path,\"Ranks.xlsx\"),engine='openpyxl',index=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dictionary\n# Target output\n'''\n\n{\n\"knnor_smote\":{\n    \"CalibratedClassifierCV\":{\n        \"auc\":{\n            1:4,\n            2:3,\n            3:6,\n            ...\n        \n            }\n        \"f1\":{\n            1:4,\n            2:3,\n            3:6,\n            ...\n        \n            }\n            ...\n    \n        }\n        \"DecisionTreeClassifier\":{\n        ...\n        }\n\n    }\n\"SMOTE_IPF\"\"{\n\n    }\n\n\n}\n\n\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"great_dict={}\nfor sampler in samplers:\n    if sampler not in great_dict:\n        great_dict[sampler]={}\n\n    for classifier in classifiers:\n        if classifier not in great_dict[sampler]:\n            great_dict[sampler][classifier]={}\n        for metric in metrics:\n            if metric not in great_dict[sampler][classifier]:\n                great_dict[sampler][classifier][metric]={}\n            # create fillers for rank counts\n            for i in range(1,len(samplers)+1):\n                if i not in great_dict[sampler][classifier][metric]:\n                    great_dict[sampler][classifier][metric][i]=0\n                newdf = df[(df[\"sampler\"] == sampler) & (df[\"classifier\"] == classifier)]\n            for index,row in newdf.iterrows():\n                rank=int(row[metric+\"_rank\"])\n                great_dict[sampler][classifier][metric][rank]+=1\n\n                \n            \n            \n                ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Following is to extract the top 5 oversamplers that were in top 3 consistently\n","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2,2,figsize=(10,10))\nfig.suptitle('Count of rank for 4 metrics')\n# for metric in metrics:\nimg_labels=['a','b','c','d']\nfor u in range(2):\n    for v in range(2):\n        metric=metrics[u*2+v]\n#         print(metric)\n        for sampler in samplers:\n#             print(sampler)    \n            rank_counter=[0 for i in range(len(samplers)+1)]\n            for classifier in classifiers:\n#                 print(classifier)\n                for rank in range(1,len(samplers)+1):\n                    rank_counter[rank]+=great_dict[sampler][classifier][metric][rank]\n#             print(rank_counter)\n            x=[i for i in range(len(rank_counter))]\n            if sampler == \"KNNOR\":\n                axs[u,v].plot(x[1:],rank_counter[1:], color='red',linewidth=1.2,linestyle='--',label=\"KNNOR\")\n            elif sampler == \"polynom_fit_SMOTE\":\n                axs[u,v].plot(x[1:],rank_counter[1:], color='blue',linewidth=1.2,linestyle='--',label=\"polynom_fit_SMOTE\")\n            elif sampler == \"ProWSyn\":\n                axs[u,v].plot(x[1:],rank_counter[1:], color='yellow',linewidth=1.2,linestyle='--',label=\"ProWSyn\")\n            elif sampler == \"SMOTE_IPF\":\n                axs[u,v].plot(x[1:],rank_counter[1:], color='green',linewidth=1.2,linestyle='--',label=\"SMOTE_IPF\")                \n            elif sampler == \"Lee\":\n                axs[u,v].plot(x[1:],rank_counter[1:], color='orange',linewidth=1.2,linestyle='--',label=\"Lee\")                \n                \n#                 axs[u,v].legend()\n            elif sampler == \"SMOTE\":\n                axs[u,v].plot(x[1:],rank_counter[1:], color='black',linewidth=0.8,label=\"Others\")        \n            else:\n                axs[u,v].plot(x[1:],rank_counter[1:], color='black',linewidth=0.8)\n            axs[u,v].set_xlabel(\"Ranks\")\n            axs[u,v].set_ylabel(\"Frequency\")\n            axs[u,v].set_title(img_labels[u*2+v]+'. Ranking Frequency for metric '+str(metric))\n        axs[u,v].legend()\n\nfig.tight_layout(pad=1.0)\nplt.savefig(os.path.join(\"../results\",\"Running_All.svg\"))\nplt.savefig(os.path.join(\"../results\",\"Running_All.jpg\"))\n\nplt.show()\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate average ranking","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"samplers=df[\"sampler\"].unique()\ncount_samplers=len(samplers)\nprint(\"Number of samplers:\",count_samplers,\"\\n\",samplers)\n\nclassifiers=df[\"classifier\"].unique()\ncount_classifiers=len(classifiers)\nprint(\"Number of classifiers:\",count_classifiers,\"\\n\",classifiers)\n\ndb_names=df[\"db_name\"].unique()\ncount_db_names=len(db_names)\nprint(\"Number of db_names:\",count_db_names,\"\\n\",db_names)\n\nmetrics=list(df.columns[-4:])\ncount_metrics=len(metrics)\nprint(\"Number of metrics:\",count_metrics,\"\\n\",metrics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nsamplers\nclassifiers\ndb_names\nmetrics\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_dict={}\nfor metric in metrics:\n    print(metric)\n    my_dict[metric]={}\n    \n    for classifier in classifiers:    \n        \n        list_vals=[]\n        print(classifier)        \n        for sampler in samplers:        \n            l=[sampler]\n            newdf = df[(df[\"sampler\"] == sampler) & (df[\"classifier\"] == classifier)]\n            avg=round(newdf[metric].mean(),4)\n            l.append(avg)\n            list_vals.append(l)\n        list_vals=sorted(list_vals, key=lambda x: x[1],reverse=True)\n        print(list_vals)\n        my_dict[metric][classifier]=list_vals\n        print()        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Below dict has been used to keep top 10 results table\n#### The big table with 4 sub tables etc etc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for metric in my_dict.keys():\n    print(metric)\n    for classifier in my_dict[metric]:\n        print(classifier)\n#         print(my_dict[metric][classifier])\n        for i in range(0,len(my_dict[metric][classifier])):\n            print(str(i+1)+\",\"+str(my_dict[metric][classifier][i][0])+\",\"+str(my_dict[metric][classifier][i][1]))\n        vals=[-1,-1]\n        for sampler,val in my_dict[metric][classifier]:\n            if sampler == \"SMOTE\":\n                vals[0]=val\n            elif sampler ==\"NoSMOTE\":\n                vals[1]=val\n        print(\"BL,SMOTE,\",vals[0])\n        print(\"BL,NoSMOTE,\",vals[1])\n        print(\"********************\")\n    print(\"*******________***********\")\n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below is for top performer table","metadata":{}},{"cell_type":"code","source":"data={}\ndata[\"sampler\"]=[]\nfor metric in metrics:\n    data[\"avg_\"+str(metric)]=[]\n    \n\nfor sampler in samplers:\n    print(sampler)\n    data[\"sampler\"].append(sampler)\n    newdf = df[(df[\"sampler\"] == sampler)]\n    for metric in metrics:\n        print(metric)\n        avg=round(newdf[metric].mean(),4)\n        print(avg)\n        data[\"avg_\"+str(metric)].append(avg)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_df=pd.DataFrame(data=data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_df.head()\navg_df.to_excel(os.path.join(cache_path,'AvgRanks.xlsx'),engine='openpyxl',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}