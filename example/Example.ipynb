{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Running KNNOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "\n",
    "# from knnor import data_augment\n",
    "# knnor=data_augment.KNNOR()\n",
    "\n",
    "from knnor import data_augment\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of the target variable [0 1]\n",
      "Counts of the target variable : [212 357]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_breast_cancer()\n",
    "\n",
    "(unique, counts) = np.unique(dataset['target'], return_counts=True)\n",
    "\n",
    "print('Unique values of the target variable', unique)\n",
    "print('Counts of the target variable :', counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape= (569, 30) (569,)\n",
      "Original distribution:\n",
      "0: 212\n",
      "1: 357\n"
     ]
    }
   ],
   "source": [
    "X=dataset[\"data\"]\n",
    "y=dataset[\"target\"]\n",
    "\n",
    "print(\"Original shape=\",X.shape,y.shape)\n",
    "elements_count = collections.Counter(y)\n",
    "# printing the element and the frequency\n",
    "print(\"Original distribution:\")\n",
    "for key, value in elements_count.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Augmentation without any parameters\n",
    "\n",
    "The algorithm calculates the parameters depending on the data\n",
    "\n",
    "Final result will give an equal number of minority and majority data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 1 [0, 1]\n",
      "Minority label 0\n",
      "0 [1]\n",
      "(357, 30) (212, 30) (569, 30)\n",
      "(569, 30) (145, 30)\n",
      "Shape after augmentation (714, 30) (714,)\n",
      "Final distribution:\n",
      "0: 357\n",
      "1: 357\n"
     ]
    }
   ],
   "source": [
    "knnor=data_augment.KNNOR()\n",
    "X_new,y_new,_,_=knnor.fit_resample(X,y)\n",
    "print(\"Shape after augmentation\",X_new.shape,y_new.shape)\n",
    "elements_count = collections.Counter(y_new)\n",
    "# printing the element and the frequency\n",
    "print(\"Final distribution:\")\n",
    "for key, value in elements_count.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation with user defined parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 1 [0, 1]\n",
      "Minority label 0\n",
      "0 [1]\n",
      "(357, 30) (212, 30) (569, 30)\n",
      "(569, 30) (502, 30)\n",
      "Shape after augmentation (1071, 30) (1071,)\n",
      "Final distribution:\n",
      "0: 714\n",
      "1: 357\n"
     ]
    }
   ],
   "source": [
    "X_new,y_new,_,_=knnor.fit_resample(X,y,\n",
    "                              num_nbrs=10, # the number of neighbors that will be used for generation of each artificial point\n",
    "                              max_dist=0.01, # the maximum distance at which the new point will be placed\n",
    "                              prop_minority=0.3, # proportion of the minority population that will be used to generate the artificial point\n",
    "                              proportion=2 # final number of minority datapoints\n",
    "                               # example, if num majority =15 and num minority =5, \n",
    "#                                putting final_proportion as 1 will add 10 artificial minority points\n",
    "                              )\n",
    "print(\"Shape after augmentation\",X_new.shape,y_new.shape)\n",
    "elements_count = collections.Counter(y_new)\n",
    "# printing the element and the frequency\n",
    "print(\"Final distribution:\")\n",
    "for key, value in elements_count.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Testing with benchmark datasets against state-of-art oversamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os.path\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from smote_variant import MLPClassifierWrapper\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "\n",
    "import imbalanced_databases as imbd\n",
    "# library containing the imbalanced datasets\n",
    "# datasets will be present in the \"/data\" folder\n",
    "# in case the data is not already there\n",
    "# the above library will be used to download and save as pickle\n",
    "\n",
    "\n",
    "import smote_variant as sv\n",
    "# the library containing oversampling code\n",
    "# includes first version of knnor as well\n",
    "# as other state of art oversamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the cache_path which is used for caching during the evaluation\n",
    "print(\"At\",os.getcwd())\n",
    "cache_path= os.path.join(os.path.dirname(os.getcwd()), 'results')\n",
    "print(cache_path)\n",
    "if not os.path.exists(cache_path):\n",
    "    os.makedirs(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specifying the classifiers used for evaluation\n",
    "\n",
    "# instantiate classifiers\n",
    "\n",
    "# Support Vector Classifiers with 6 parameter combinations\n",
    "sv_classifiers= [CalibratedClassifierCV(LinearSVC(C=1.0, penalty='l1', loss= 'squared_hinge', dual= False)),\n",
    "                 CalibratedClassifierCV(LinearSVC(C=1.0, penalty='l2', loss= 'hinge', dual= True)),\n",
    "                 CalibratedClassifierCV(LinearSVC(C=1.0, penalty='l2', loss= 'squared_hinge', dual= False)),\n",
    "                 CalibratedClassifierCV(LinearSVC(C=10.0, penalty='l1', loss= 'squared_hinge', dual= False)),\n",
    "                 CalibratedClassifierCV(LinearSVC(C=10.0, penalty='l2', loss= 'hinge', dual= True)),\n",
    "                 CalibratedClassifierCV(LinearSVC(C=10.0, penalty='l2', loss= 'squared_hinge', dual= False))]\n",
    "\n",
    "# Multilayer Perceptron Classifiers with 6 parameter combinations\n",
    "mlp_classifiers= []\n",
    "for x in itertools.product(['relu', 'logistic'], [1.0, 0.5, 0.1]):\n",
    "    mlp_classifiers.append(MLPClassifierWrapper(activation= x[0], hidden_layer_fraction= x[1]))\n",
    "\n",
    "# Nearest Neighbor Classifiers with 18 parameter combinations\n",
    "nn_classifiers= []\n",
    "for x in itertools.product([3, 5, 7], ['uniform', 'distance'], [1, 2, 3]):\n",
    "    nn_classifiers.append(KNeighborsClassifier(n_neighbors= x[0], weights= x[1], p= x[2]))\n",
    "\n",
    "# Decision Tree Classifiers with 6 parameter combinations\n",
    "dt_classifiers= []\n",
    "for x in itertools.product(['gini', 'entropy'], [None, 3, 5]):\n",
    "    dt_classifiers.append(DecisionTreeClassifier(criterion= x[0], max_depth= x[1]))\n",
    "\n",
    "classifiers= []\n",
    "classifiers.extend(sv_classifiers)\n",
    "classifiers.extend(mlp_classifiers)\n",
    "classifiers.extend(nn_classifiers)\n",
    "classifiers.extend(dt_classifiers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment from below list to include\n",
    "# more augmentation algorithms\n",
    "oversamplers= [    \n",
    "    sv.SMOTE,\n",
    "    sv.NoSMOTE,\n",
    "    sv.KNNOR_SMOTE,\n",
    "    sv.polynom_fit_SMOTE,\n",
    "    sv.ProWSyn,\n",
    "    sv.SMOTE_IPF,\n",
    "    sv.Lee,\n",
    "    sv.SMOBD,\n",
    "    sv.G_SMOTE,\n",
    "    sv.CCR,\n",
    "    sv.LVQ_SMOTE,\n",
    "    sv.Assembled_SMOTE,    \n",
    "    sv.SMOTE_TomekLinks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if pickle file for data exists\n",
    "if os.path.isfile('../data/sampled_datasets_below_1000.p'):\n",
    "    sampled_datasets_below_1000 = pickle.load( open( \"../data/sampled_datasets_below_1000.p\", \"rb\" ) )\n",
    "else:\n",
    "    sampled_datasets_below_1000=[\n",
    "        imbd.load_ecoli_0_1_3_7_vs_2_6,\n",
    "        imbd.load_pima,\n",
    "        imbd.load_cm1,\n",
    "        imbd.load_vowel0,\n",
    "        imbd.load_glass_0_1_6_vs_2,\n",
    "        imbd.load_yeast_0_5_6_7_9_vs_4,\n",
    "        imbd.load_yeast_1_vs_7,\n",
    "        imbd.load_ecoli_0_3_4_7_vs_5_6,\n",
    "        imbd.load_cleveland_0_vs_4,\n",
    "        imbd.load_iris0,\n",
    "        imbd.load_ecoli_0_6_7_vs_5,\n",
    "        imbd.load_winequality_white_3_vs_7,\n",
    "        imbd.load_ecoli_0_1_4_7_vs_2_3_5_6,\n",
    "        imbd.load_ecoli_0_3_4_vs_5,\n",
    "        imbd.load_glass0,\n",
    "        imbd.load_habarman,\n",
    "        imbd.load_glass_0_1_2_3_vs_4_5_6,\n",
    "        imbd.load_ecoli_0_6_7_vs_3_5,\n",
    "        imbd.load_poker_9_vs_7,\n",
    "    ]\n",
    "    pickle.dump( sampled_datasets_below_1000, open( \"../data/sampled_datasets_below_1000.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_files=len(sampled_datasets_below_1000)\n",
    "print(\"total files=\",count_files)\n",
    "\n",
    "# comment below lines when running full version\n",
    "samples_used=1\n",
    "sampled_datasets_below_1000=sampled_datasets_below_1000[:samples_used]\n",
    "print(\"files used in this round of experiment\",len(sampled_datasets_below_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start=int(time.time())\n",
    "max_samp_par_comb=1 #50\n",
    "# change above to a higher value\n",
    "# for more trial runs\n",
    "\n",
    "\n",
    "results= sv.evaluate_oversamplers(datasets= sampled_datasets_below_1000,\n",
    "                                    samplers= oversamplers,\n",
    "                                    classifiers=classifiers,\n",
    "                                    cache_path= cache_path,\n",
    "                                    n_jobs= 12,\n",
    "                                    max_samp_par_comb= max_samp_par_comb)\n",
    "duration=int(time.time())-start\n",
    "print(\"Time taken = \",duration,\" seconds\")\n",
    "results.to_csv(os.path.join(cache_path,\"Results.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate time taken on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speed=results.groupby('sampler')['runtime'].mean()\n",
    "df_speed=df_speed.sort_values()\n",
    "df_speed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_cols=[\"db_name\",\"classifier\",\"sampler\",\"auc\",\"f1\",\"p_top20\",\"gacc\"]\n",
    "\n",
    "df = results[interesting_cols]\n",
    "df=df.replace(\"KNNOR_SMOTE\",\"KNNOR\")\n",
    "samplers=df[\"sampler\"].unique()\n",
    "count_samplers=len(samplers)\n",
    "print(\"Number of samplers:\",count_samplers,\"\\n\",samplers)\n",
    "\n",
    "classifiers=df[\"classifier\"].unique()\n",
    "count_classifiers=len(classifiers)\n",
    "print(\"Number of classifiers:\",count_classifiers,\"\\n\",classifiers)\n",
    "\n",
    "\n",
    "db_names=df[\"db_name\"].unique()\n",
    "count_db_names=len(db_names)\n",
    "print(\"Number of db_names:\",count_db_names,\"\\n\",db_names)\n",
    "\n",
    "\n",
    "metrics=list(df.columns[-4:])\n",
    "count_metrics=len(metrics)\n",
    "print(\"Number of metrics:\",count_metrics,\"\\n\",metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "#     df[metric+\"_rank\"]=[0 for i in range(df.shape[0])]\n",
    "    df[metric+\"_rank\"]=df.groupby(['db_name','classifier'])[metric].rank(ascending=False,method=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(os.path.join(cache_path,\"Ranks.xlsx\"),engine='openpyxl',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "# Target output\n",
    "'''\n",
    "\n",
    "{\n",
    "\"knnor_smote\":{\n",
    "    \"CalibratedClassifierCV\":{\n",
    "        \"auc\":{\n",
    "            1:4,\n",
    "            2:3,\n",
    "            3:6,\n",
    "            ...\n",
    "        \n",
    "            }\n",
    "        \"f1\":{\n",
    "            1:4,\n",
    "            2:3,\n",
    "            3:6,\n",
    "            ...\n",
    "        \n",
    "            }\n",
    "            ...\n",
    "    \n",
    "        }\n",
    "        \"DecisionTreeClassifier\":{\n",
    "        ...\n",
    "        }\n",
    "\n",
    "    }\n",
    "\"SMOTE_IPF\"\"{\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_dict={}\n",
    "for sampler in samplers:\n",
    "    if sampler not in great_dict:\n",
    "        great_dict[sampler]={}\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        if classifier not in great_dict[sampler]:\n",
    "            great_dict[sampler][classifier]={}\n",
    "        for metric in metrics:\n",
    "            if metric not in great_dict[sampler][classifier]:\n",
    "                great_dict[sampler][classifier][metric]={}\n",
    "            # create fillers for rank counts\n",
    "            for i in range(1,len(samplers)+1):\n",
    "                if i not in great_dict[sampler][classifier][metric]:\n",
    "                    great_dict[sampler][classifier][metric][i]=0\n",
    "                newdf = df[(df[\"sampler\"] == sampler) & (df[\"classifier\"] == classifier)]\n",
    "            for index,row in newdf.iterrows():\n",
    "                rank=int(row[metric+\"_rank\"])\n",
    "                great_dict[sampler][classifier][metric][rank]+=1\n",
    "\n",
    "                \n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following is to extract the top 5 oversamplers that were in top 3 consistently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2,figsize=(10,10))\n",
    "fig.suptitle('Count of rank for 4 metrics')\n",
    "# for metric in metrics:\n",
    "img_labels=['a','b','c','d']\n",
    "for u in range(2):\n",
    "    for v in range(2):\n",
    "        metric=metrics[u*2+v]\n",
    "#         print(metric)\n",
    "        for sampler in samplers:\n",
    "#             print(sampler)    \n",
    "            rank_counter=[0 for i in range(len(samplers)+1)]\n",
    "            for classifier in classifiers:\n",
    "#                 print(classifier)\n",
    "                for rank in range(1,len(samplers)+1):\n",
    "                    rank_counter[rank]+=great_dict[sampler][classifier][metric][rank]\n",
    "#             print(rank_counter)\n",
    "            x=[i for i in range(len(rank_counter))]\n",
    "            if sampler == \"KNNOR\":\n",
    "                axs[u,v].plot(x[1:],rank_counter[1:], color='red',linewidth=1.2,linestyle='--',label=\"KNNOR\")\n",
    "            elif sampler == \"polynom_fit_SMOTE\":\n",
    "                axs[u,v].plot(x[1:],rank_counter[1:], color='blue',linewidth=1.2,linestyle='--',label=\"polynom_fit_SMOTE\")\n",
    "            elif sampler == \"ProWSyn\":\n",
    "                axs[u,v].plot(x[1:],rank_counter[1:], color='yellow',linewidth=1.2,linestyle='--',label=\"ProWSyn\")\n",
    "            elif sampler == \"SMOTE_IPF\":\n",
    "                axs[u,v].plot(x[1:],rank_counter[1:], color='green',linewidth=1.2,linestyle='--',label=\"SMOTE_IPF\")                \n",
    "            elif sampler == \"Lee\":\n",
    "                axs[u,v].plot(x[1:],rank_counter[1:], color='orange',linewidth=1.2,linestyle='--',label=\"Lee\")                \n",
    "                \n",
    "#                 axs[u,v].legend()\n",
    "            elif sampler == \"SMOTE\":\n",
    "                axs[u,v].plot(x[1:],rank_counter[1:], color='black',linewidth=0.8,label=\"Others\")        \n",
    "            else:\n",
    "                axs[u,v].plot(x[1:],rank_counter[1:], color='black',linewidth=0.8)\n",
    "            axs[u,v].set_xlabel(\"Ranks\")\n",
    "            axs[u,v].set_ylabel(\"Frequency\")\n",
    "            axs[u,v].set_title(img_labels[u*2+v]+'. Ranking Frequency for metric '+str(metric))\n",
    "        axs[u,v].legend()\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "plt.savefig(os.path.join(\"../results\",\"Running_All.svg\"))\n",
    "plt.savefig(os.path.join(\"../results\",\"Running_All.jpg\"))\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers=df[\"sampler\"].unique()\n",
    "count_samplers=len(samplers)\n",
    "print(\"Number of samplers:\",count_samplers,\"\\n\",samplers)\n",
    "\n",
    "classifiers=df[\"classifier\"].unique()\n",
    "count_classifiers=len(classifiers)\n",
    "print(\"Number of classifiers:\",count_classifiers,\"\\n\",classifiers)\n",
    "\n",
    "db_names=df[\"db_name\"].unique()\n",
    "count_db_names=len(db_names)\n",
    "print(\"Number of db_names:\",count_db_names,\"\\n\",db_names)\n",
    "\n",
    "metrics=list(df.columns[-4:])\n",
    "count_metrics=len(metrics)\n",
    "print(\"Number of metrics:\",count_metrics,\"\\n\",metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "samplers\n",
    "classifiers\n",
    "db_names\n",
    "metrics\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict={}\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    my_dict[metric]={}\n",
    "    \n",
    "    for classifier in classifiers:    \n",
    "        \n",
    "        list_vals=[]\n",
    "        print(classifier)        \n",
    "        for sampler in samplers:        \n",
    "            l=[sampler]\n",
    "            newdf = df[(df[\"sampler\"] == sampler) & (df[\"classifier\"] == classifier)]\n",
    "            avg=round(newdf[metric].mean(),4)\n",
    "            l.append(avg)\n",
    "            list_vals.append(l)\n",
    "        list_vals=sorted(list_vals, key=lambda x: x[1],reverse=True)\n",
    "        print(list_vals)\n",
    "        my_dict[metric][classifier]=list_vals\n",
    "        print()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below dict has been used to keep top 10 results table\n",
    "#### The big table with 4 sub tables etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in my_dict.keys():\n",
    "    print(metric)\n",
    "    for classifier in my_dict[metric]:\n",
    "        print(classifier)\n",
    "#         print(my_dict[metric][classifier])\n",
    "        for i in range(0,len(my_dict[metric][classifier])):\n",
    "            print(str(i+1)+\",\"+str(my_dict[metric][classifier][i][0])+\",\"+str(my_dict[metric][classifier][i][1]))\n",
    "        vals=[-1,-1]\n",
    "        for sampler,val in my_dict[metric][classifier]:\n",
    "            if sampler == \"SMOTE\":\n",
    "                vals[0]=val\n",
    "            elif sampler ==\"NoSMOTE\":\n",
    "                vals[1]=val\n",
    "        print(\"BL,SMOTE,\",vals[0])\n",
    "        print(\"BL,NoSMOTE,\",vals[1])\n",
    "        print(\"********************\")\n",
    "    print(\"*******________***********\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is for top performer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "data[\"sampler\"]=[]\n",
    "for metric in metrics:\n",
    "    data[\"avg_\"+str(metric)]=[]\n",
    "    \n",
    "\n",
    "for sampler in samplers:\n",
    "    print(sampler)\n",
    "    data[\"sampler\"].append(sampler)\n",
    "    newdf = df[(df[\"sampler\"] == sampler)]\n",
    "    for metric in metrics:\n",
    "        print(metric)\n",
    "        avg=round(newdf[metric].mean(),4)\n",
    "        print(avg)\n",
    "        data[\"avg_\"+str(metric)].append(avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df=pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df.head()\n",
    "avg_df.to_excel(os.path.join(cache_path,'AvgRanks.xlsx'),engine='openpyxl',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
